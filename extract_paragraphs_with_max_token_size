#A good example of extracting paragraph to feed retriever properly for RAG use cases. 

import pandas as pd
from pathlib import Path
from tqdm import tqdm
import numpy as np
from zemberek import TurkishSentenceExtractor, TurkishSentenceNormalizer, TurkishMorphology
from transformers import AutoTokenizer
import re


tokenizer = AutoTokenizer.from_pretrained(Path("path_to_tokenizer").expanduser())

sentence_extractor = TurkishSentenceExtractor()


def extract_paragraphs_with_max_token_size(context, tokenizer, max_token_size, sentence_extractor, split_with_max=True):
    num_tokens = tokenizer(context, return_tensors="np")["input_ids"].size
    if num_tokens < max_token_size:
        return [context]
    else:
        contexts = []
        sentences = sentence_extractor.from_paragraph(context)
        sentences = [sentences[0]] + [" " + sntc for sntc in sentences[1:]]
        sentence_tokens = tokenizer(sentences, return_tensors="np")["input_ids"]
        cumulative_num_tokens = np.cumsum(np.array(list(map(len, sentence_tokens))))
        num_parts = int(np.ceil(num_tokens / max_token_size))
        if split_with_max:
            num_tokens_per_part = max_token_size
        else:
            num_tokens_per_part = int(np.ceil(num_tokens / num_parts))
        part_start_index = 0
        for part_no in range(num_parts-1):
            part_end_index = np.argmax(cumulative_num_tokens > num_tokens_per_part)
            contexts.append("".join(sentences[part_start_index:part_end_index]))
            part_start_index = part_end_index
            cumulative_num_tokens -= cumulative_num_tokens[part_end_index-1]
        contexts.append("".join(sentences[part_start_index:]))
        return contexts

 

strange_chars= ["�"]

paragraphs = []

for filepath in tqdm(glob(Path("html_book").expanduser().as_posix() + "/*")):
    page = urlopen(f"file://{filepath}")
    html = page.read().decode("utf-8")
    soup = BeautifulSoup(html, 'html.parser')
    for prgs in soup.find_all('p'):
        cnt = " ".join(prgs.text.replace("\r\n", " ").replace(u'\xa0', u' ').split())
        cnt = cnt.translate({ord("Đ"): u'İ', ord("Ý"): u'İ', ord("ý"): u'ı', ord("Þ"): u'Ş', ord("þ"): u'ş', ord("Ð"): u'Ğ', ord("ð"): u'ğ'})
        cnt = re.sub("(?<=[a-zA-ZöÖğĞıİçÇüÜşŞ])-(?=[a-zA-ZöÖğĞıİçÇüÜşŞ])", "", cnt)
        if len(cnt.split(" ")) > 100 and (not any([ch in cnt for ch in strange_chars])):
            paragraphs.extend(extract_paragraphs_with_max_token_size(cnt, tokenizer, 4095, sentence_extractor, split_with_max=False))
 
final_df = pd.Series(paragraphs, name="context").to_frame()
